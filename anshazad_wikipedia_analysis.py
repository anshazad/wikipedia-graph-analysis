# -*- coding: utf-8 -*-
"""anshazad_wikipedia_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mKGQLBFOAdTEC3zaDUWefu17xD2jxAGF

**Name: Ansh Azad (ID: 12040240)**

## Graph Code
"""

class Vertex:
    def __init__(self, node):  
        self.id = node
        self.label=None
        self.adjacent = {}
        self.keywords = None
        self.tags= None
        self.bag_vector = None
        self.TFIDF_vector = None
        self.degreeCentrality= None
    
     #prints nodes and its neighbours
    def __str__(self):     
        return str(self.id) + ' adjacent: ' + str([x.id for x in self.adjacent])
    
    #function to add neighbours to a node
    def add_neighbor(self, neighbor, weight=0):
        self.adjacent[neighbor] = weight
    
    #returns all the neighbours links
    def get_connections(self):
        return self.adjacent.keys()  

    #returns the node id
    def get_id(self):
        return self.id
    
    #returns the weight of the edge between the node and its particular neighbour
    def get_weight(self, neighbor):
        return self.adjacent[neighbor]
    
    #adds the keywords to the node
    def add_Keywords_tags(self,keywords,tags):
        self.keywords=keywords
        self.tags=tags
    
    #adds the features to the node
    def add_features(self, X, Y):
        self.bag_vector = X
        self.TFIDF_vector = Y
    
    #function to print the feature vectors of the node
    def get_features(self):
        print("Feature 1: Bag of words feature vector")
        print(self.bag_vector)
        print("Feature 2: TF-IDF feature vector")
        print(self.TFIDF_vector)
    
    #function to return the degree centrality of the node
    def get_DegreeCentrality(self,total):
        l=list(self.adjacent.keys())
        x=len(l)
        return x/(total-1)
    
    #funtion to return the clustering coefficient of the node
    def get_Clustering_coefficient(self):
        l=list(self.adjacent.keys())
        c=0
        x=len(l)
        if x==0 or x==1:
            return 0
        else:
            for i in range(x):
                for j in range(x-1):
                    if l[i] in (l[j+1].adjacent.keys()):
                        c+=1
            return c/(x*(x-1))
                    

class Graph:   #class of the graph
    def __init__(self): 
        
        #dictionary storing the links and the vertex corresponding to the link
        
        self.vert_dict = {}  
        self.num_vertices = 0

    def __iter__(self):    
        
        #returns an iterable to parse through the graph
        return iter(self.vert_dict.values())

    #adds a vertex to the graph which is stored in the dictionary
    def add_vertex(self, link):  
        self.num_vertices = self.num_vertices + 1 
        new_vertex = Vertex(link)
        self.vert_dict[link] = new_vertex
        #return new_vertex
    
    #adds the NLP features to the node
    def add_feature(self,link,text):
        node=self.vert_dict[link]
        words=(remove_stop_words(tokenize(text)))
        keywords,tags= keywords_tags(text,words)
        node.add_Keywords_tags(keywords,tags)
        X,Y= NLP_Feature_Vectors(text)
        node.add_features(X,Y)
        return None

    #returns the vertex corresponding to the link n
    def get_vertex(self, n):
        if n in self.vert_dict:
            return self.vert_dict[n]
        else:
            print ("Vertex not found")
            return None

    #adds an edge between two links (edges are kept track of by each individual nodes adjacency list)
    def add_edge(self, frm_link, to_link, cost = 0):
        if frm_link not in self.vert_dict.keys():
            self.add_vertex(frm_link)
        if to_link not in self.vert_dict.keys():
            self.add_vertex(to_link)

        self.vert_dict[frm_link].add_neighbor(self.vert_dict[to_link], cost)
        self.vert_dict[to_link].add_neighbor(self.vert_dict[frm_link], cost)

    #returns a list of all the vertices
    def get_vertices(self):
        return self.vert_dict.keys()
    
    def Degree_Centrality(self):
        for link,vertex in self.vert_dict.items():
            print(f"{link} \t {vertex.get_DegreeCentrality(self.num_vertices)}")
        return None
    
    def BFS(self):        #graph traversal algorithm
        open_queue=[]
        closed=[]
        for link,vertex in self.vert_dict.items():
            if((link not in open_queue) or (link not in closed)):
                open_queue.append(link)
                neighbours=list(vertex.adjacent.keys())
                for j in neighbours:
                    if((j not in open_queue) or (j not in closed)):
                        open_queue.append(j)
                closed.append(open_queue.pop(open_queue.index(link)))
        return closed
            
    def Clustering_coefficient(self):
        for link,vertex in self.vert_dict.items():
            print(f"{link} \t {vertex.get_Clustering_coefficient()}")

"""## Natural Language Processing"""

pip install nltk

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from nltk.tokenize import sent_tokenize

stop_words = set(stopwords.words('english'))
print(stop_words)

def tokenize(text):  #gets the raw text extracted from wikipedia and converts it to a list of words 
    return word_tokenize(text)

text="Hello my name is Ansh Azad and I am 19 years old. I am from New Delhi. I joined IIT BHilai in November 2020 and have been here ever since. "
nltk.download('punkt')
print(tokenize(text))

def remove_stop_words(words): #takes the list of words and removes the stop words and words with length less than 2
    l=[word for word in words if (word not in stop_words)]
    l1=[word for word in l if len(word)>2]
    return l1

print(remove_stop_words(tokenize(text)))

from operator import itemgetter
import math

def TF(words):  #sending tokenized and stop word free list of words
    no_of_words = len(words)
#     print(no_of_words)
    tf_score = {}
    for each_word in words:
        if each_word in tf_score:
            tf_score[each_word] += 1
        else:
            tf_score[each_word] = 1
    # Dividing by no_of_words for each dictionary element
    tf_score.update((x, y/int(no_of_words)) for x, y in tf_score.items())
    return (tf_score)

def check_sent(word, sentences):        #returns the number of sentences the word is in
    final = [all([w in x for w in word]) for x in sentences] 
    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]
    return int(len(sent_len))

def IDF(text,words):                    #takes the raw text and the words list
    sentences = sent_tokenize(text)
    no_of_sentences = len(sentences)
#     print(no_of_sentences)
    idf_score = {}
    for each_word in words:
        if each_word in idf_score:
            idf_score[each_word] = check_sent(each_word, sentences)
        else:
            idf_score[each_word] = 1

    # Performing a log and divide
    idf_score.update((x, math.log(int(no_of_sentences)/y)) for x, y in idf_score.items())

    return(idf_score)
    
def keywords_tags(text,words):
    tf_score=TF(words)
    idf_score=IDF(text,words)
    tf_idf_score = {key: tf_score[key] * idf_score.get(key, 0) for key in tf_score.keys()}
    print(tf_idf_score)
    keywords = dict(sorted(tf_idf_score.items(), key = itemgetter(1), reverse = True)[:10])
    tags= dict(sorted(tf_idf_score.items(), key = itemgetter(1), reverse = True)[:4])
    return (list(keywords.keys())),(list(tags.keys()))

text="Hello my name is Ansh Azad and I am 19 years old. I am from New Delhi. I joined IIT BHilai in November 2020 and have been here ever since. "
words=(remove_stop_words(tokenize(text)))
X,Y=(keywords_tags(text,words))
print(X)
print(Y)

"""## Feature vector generation- Bag of words vector, TF-IDF vector"""

import numpy as np
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer


def NLP_Feature_Vectors(text):   #Code taken from the NLP tutorial done in class
    ps = PorterStemmer()
    wordnet=WordNetLemmatizer()
    sentences = nltk.sent_tokenize(text) #Tokenizing 
    corpus = []
    for i in range(len(sentences)):
        review = re.sub('[^a-zA-Z]', ' ', sentences[i]) 
        review = review.lower() #Lowering the sentence 
        review = review.split() #Splitting it into words
        review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] #Applying stemming
        review = ' '.join(review)
        corpus.append(review)    
        
    # Creating Bag of Words model
    from sklearn.feature_extraction.text import CountVectorizer
    cv = CountVectorizer(max_features = 10)
    X = cv.fit_transform(corpus).toarray()  #bag of words feature vector
    
    # Creating TF-IDF model
    from sklearn.feature_extraction.text import TfidfVectorizer
    tv = TfidfVectorizer(max_features=50)
    Y = tv.fit_transform(corpus).toarray() #TF-IDF feature vector
    sum1=np.zeros(len(Y[0]))
    for i in Y:
        sum1+=i
    sum1/=len(Y)
    
    
#     print(len(sentences)) #Checking the number of sentences in the paragraph 
#     print(X) # Printing the final feature matrix 
#     print(X.shape) # Shape of the feature matrix 
#     print(corpus) # Printing the processed text 
#     print(sentences) # Printing the original text
#     print(Y)
#     print(Y.shape)
    return X,sum1

"""## Reading data and Creating Graph"""

import pandas as pd
df=pd.read_csv("b_bit.csv")
df.head()

df.drop(columns=["out_links"])
df.head()

df['link'][5]

g=Graph()

for i in range(len(df['title'])):
    link=df['link'][i]
    text=df['article'][i]
    out=eval(df['out_link'][i])
    g.add_vertex(link)
    g.add_feature(link,text)
    for j in out:
        g.add_edge(link,j)

x=g.vert_dict['https://en.wikipedia.org/wiki/List_of_statistics_journals']

print(x)

type(x.adjacent)

x.get_features()

print(x.keywords)

print(x.tags)

len(x.TFIDF_vector)

x.bag_vector

print(list(x.adjacent.keys())[0])

"""# Bfs traversal on whole graph starting from the first node"""

for i in l:
    print("{}:{}".format((l.index(i)+1),i))

len(l)

g.Degree_Centrality()

"""## Visualizing the Graph"""

import networkx as nx
import matplotlib.pyplot as plt

class GraphVisualization: 
   
    def __init__(self):
          
        # visual is a list which stores all 
        # the set of edges that constitutes a
        # graph
        self.visual = []
          
    # addEdge function inputs the vertices of an
    # edge and appends it to the visual list
    def addEdge(self, a, b):
        temp = [a, b]
        self.visual.append(temp)
          
    # In visualize function G is an object of
    # class Graph given by networkx G.add_edges_from(visual)
    # creates a graph with a given list
    # nx.draw_networkx(G) - plots the graph
    # plt.show() - displays the graph
    def visualize(self):
        G = nx.Graph()
        G.add_edges_from(self.visual)
        nx.draw_networkx(G,with_labels=False)
        plt.figure(figsize=(1000,1000))
        plt.show()

Viz_G=GraphVisualization()
for vertex in g.vert_dict.values():
    l=vertex.adjacent
    for i in l:
        Viz_G.addEdge(vertex.id,i.id)
Viz_G.visualize()

"""## Labelling the graph according to the given dataset"""

df2=pd.read_csv("labeled.csv")

df2.head()

def feature(node,link,text):
        words=(remove_stop_words(tokenize(text)))
        keywords,tags= keywords_tags(text,words)
        node.add_Keywords_tags(keywords,tags)
        X,Y= NLP_Feature_Vectors(text)
        node.add_features(X,Y)
        return None

label_lst=[]
len(df2["Link"])
c=0
for i in range(464):
    c+=1
    label=df2["Label"][i]
    link=df2["Link"][i]
    text=df2["article"][i]
    node=Vertex(link)
    feature(node,link,str(text))
    node.label=label
    label_lst.append(node)
print(c)

len(label_lst)

len(label_lst[3].TFIDF_vector)

label_lst[1].label

label_lst[0].TFIDF_vector.shape

label_lst[0].TFIDF_vector

"""# part D

"""

for i in g.vert_dict.values():
    maxi=0
    k1=i.TFIDF_vector
    for j in label_lst:
        k2=j.TFIDF_vector
        if len(k1)==len(k2):
            dot=[k1[i] * k2[i] for i in range(len(k1))]
            print(dot)
            dotp=sum(dot)
            print(dotp)
            if(dotp>maxi):
                i.label=j.label
                maxi=dotp
        else:
            pass

c=0
for link,vertex in g.vert_dict.items():
    if(vertex.label!=None):
        c+=1
        #print(link,vertex.label)
print(c)

"""## Part E: Graph Traversal and Article Ordering Algorithm """

flag = 0
node=None
inp=input("Enter a topic")
inp_link = "https://en.wikipedia.org/wiki/"+inp
for link,vertex in g.vert_dict.items():
    if inp_link==link:
        node=vertex
#         nlst=list(vertex.adjacent.keys())
        flag+=1
if flag==0:
    print("topic not found")
else:
    closed=[]
    l=label_BFS(node,closed)
    print(l)

for i in l:
    print(i.id,i.label)

def label_BFS(node,closed):   #sub graph traversal algorithm using labels
    neighbours=list(node.adjacent.keys())
    open_queue=[]
    k=[]
    for vertex in neighbours:
        if((vertex.id not in open_queue) or (vertex.id not in closed)):
            open_queue.append((vertex.label,vertex.id,vertex))
    for i in open_queue:
#         print(i[0])
#         break
        if i[0]==0 or i[0]==None:
            pass
        else:
            k.append(i)
#     print(k)
    sk = sorted(k,key=lambda x:x[0])
    print(sk)
    closed+=[sk[0][2]]
    if len(closed)==10:
        return closed
    else:
        return label_BFS(sk[0][2],closed)